enable_ansible_debug: true
# Pipeline resource configuration
## CUSTOMER_NAME:
# If needed, please go to network.pivotal.io and create an account, try to download pks and accept the EULA
# CHANGE_ME ... Pivnet token for downloading resources from Pivnet. Find this token at https://network.pivotal.io/users/dashboard/edit-profile
pivnet_token: GkEzUB1ymWNVbztUevjF 
# Ops Manager minor version to use - CHANGE_ME
opsman_major_minor_version: 2.6.5 
# Major/Minor PKS version - CHANGE_ME
pks_major_minor_version: 1.4.1    
# vCenter configuration
# vCenter host or IP
vcenter_host: 10.173.61.41
# CHANGE_ME vCenter username. If user is tied to a domain, then escape the \, example `domain\\user`
vcenter_usr: administrator@vsphere.local
# CHANGE_ME Uncomment and set if not using Vault or CredHub (vcenter_pwd)
vcenter_pwd: "VMware1!"
# CHANGE_ME vCenter datacenter where PKS will be installed
vcenter_data_center: DC1
# vCenter skip TLS cert validation; enter `1` to disable cert verification, `0` to enable verification
vcenter_insecure: 1
# vCenter CA cert at the API endpoint; enter a value if `vcenter_insecure: 0`
vcenter_ca_cert:

# Ops Manager VM configuration
# Optional - vCenter host to deploy Ops Manager in
om_vm_host: 
# CHANGE_ME vCenter datastore name to deploy Ops Manager in
om_data_store: datastoreComp
# CHANGE_ME IP or FQDN to access Ops Manager without protocol (will use https), ex: opsmgr.example.com
opsman_domain_or_ip_address: opsmgr.mylab.com
# Username for Ops Manager admin account
opsman_admin_username: admin
# CHANGE_ME Uncomment and set if not using Vault or CredHub (opsman_admin_password)
opsman_admin_password: "VMware1!"
# New for opsman 2.6 and above
# CHANGE_ME Uncomment and set if not using Vault or CredHub
om_ssh_key: "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCpzi0ZKWsC3jI56s51OTMDwY5tfesBGX/l00EyoW5Dzr2NIOFlB3HEj6+yq69TVypfKzQFtol0kCsktw3sGxhtkXPafDRr2mkq4GztRm8DyBlq8AlcVAM17jtbTHxNOLSoCsjzfr44wEZDXCeTg2ZOhMihyCw1VWBbtdQqn551vRpSqbbmaBuMje0iMEcp164qOvIl3+83SaGJeJKEpCVYu+VCzorDhe16u2VKKCiygwGyj63B3ZhwUXUatCX9ykTeW5H4NrEQAwLSRYiT4J7Y3w+gvNdBNWzrCwQ9LBFkmM275ZRa+G685ZLwIBqMYTbF30C9XWNaMbbJne/FZ+JZ0mSAD3gmeigNtAQXTZL1YXYt5qMYOooQ0X5Aa2DTcMh94Ev+RSv2As9elidQ28sluFD11ovv6X0LVrZL6onNpQaNEw0Q5KJCYHgnPecFGeasl3OMvPmeZtrGLY18lV5kZrwarB38thDEkNSmdIUhrpV3q36vZwPfo1G/TjfoxSAWql5/NdO8cAGdzsJJHYYNaRgVhMrNIoqtwCbOP75galG9E1+N5atxDSt1N12HAmzPbBC1ATgU++1RPTkw+U0ytFMvAunGZaqVPgQyM8Pe3bNZyNcIAjoYbBWX1HPY1HwD7/cuKtY8WjkD/T54DFpQgt+UiwIkZzE2aErf6L/rfw== torgersona@vmware.com" 
# Uncomment and set if not using Vault or CredHub (om_decryption_pwd)
om_decryption_pwd: "VMware1!"
# CHANGE_ME Comma-separated list of NTP Servers
om_ntp_servers: 10.188.26.119
# CHANGE_ME Comma-separated list of DNS Servers
om_dns_servers: 10.173.61.54
# CHANGE_ME Gateway PKS-MGMT network
om_gateway: 10.1.1.1
# CHANGE_ME Netmask for PKS-MGMT network
om_netmask: 255.255.255.0
# CHANGE_ME IP to assign to Ops Manager VM from PKS-MGMT logical network
om_ip: 10.1.1.2
# vCenter network name to use to deploy Ops Manager in
om_vm_network: LS-PKS-MGMT
# Name to use for Ops Manager VM
om_vm_name: opsmgr
# Disk type for Ops Manager VM (thick|thin)
opsman_disk_type: "thin"
# Whether to power on Ops Manager VM after creation
om_vm_power_state: true
# Leave it blank
om_vm_folder: 

# vCenter Cluster or Resource Pool to use to deploy Ops Manager.
# Create resource pools PKS-MGMT and PKS-K8S in vcenter prior to running pipeline
# Resource pool for PKS-MGMT vms 
om_resource_pool: PKS-MGMT 

# Storage for PKS-MGMT VMs and bosh
# CHANGE_ME ephemeral Storage names in vCenter for use by PKS
ephemeral_storage_names: datastoreComp
# CHANGE_ME Persistent Storage names in vCenter for use by PKS
persistent_storage_names: datastoreComp

# vSphere datacenter folder (such as pcf_vms) where VMs will be placed
bosh_vm_folder: "pks_vms"
# vSphere datacenter folder (such as pcf_templates) where templates will be placed
bosh_template_folder: "pks_templates"
# vSphere datastore folder (such as pcf_disk) where attached disk images will be created
bosh_disk_path: "pks_disk" 

# Trusted certificates to be deployed along with all VM's provisioned by BOSH
trusted_certificates:
# Disk type for BOSH provisioned VM. (thick|thin) 
vm_disk_type: "thick"

# AZ configuration for Ops Director
# Logical name of availability zone. No spaces or special characters.
az_1_name: PKS-MGMT
# CHANGE_ME Name of cluster in vCenter for PKS
az_1_cluster_name: COMP
# Resource pool name in vCenter for PKS
az_1_rp_name: PKS-MGMT

# Logical name of availability zone. No spaces or special characters.
az_2_name: PKS-K8S
# CHANGE_ME Name of cluster in vCenter for PKS
az_2_cluster_name: COMP
# Resource pool name in vCenter for PKS 
az_2_rp_name: PKS-K8S

# Leave blank unless creating more than two resource pools.
# Logical name of availability zone. No spaces or special characters.
az_3_name: 
# Name of cluster in vCenter for PKS
az_3_cluster_name:
# Resource pool name in vCenter for PKS
az_3_rp_name: 

# CHANGE_ME Comma-separated list of NTP servers to use for VMs deployed by BOSH
ntp_servers: time.vmware.com
# Whether to enable BOSH VM resurrector
enable_vm_resurrector: true 
# Max threads count for deploying VMs
max_threads: 30 

# Network configuration for Ops Director
# Enable or disable ICMP checks
icmp_checks_enabled: true 

# Name of the network
infra_network_name: "PKS-MGMT"
# PKS-MGMT logical switch name as seen in vsphere
infra_vsphere_network: "LS-PKS-MGMT"
# CHANGE_ME PKS-MGMT logical network CIDR
infra_nw_cidr: 10.1.1.0/24
# CHANGE_ME Infrastructure network exclusion range i.e - 10.87.0.1-10.87.0.2
infra_excluded_range: 10.1.1.1-10.1.1.2
# CHANGE_ME Infrastructure network DNS, any DNS
infra_nw_dns: 10.173.61.54
# CHANGE_ME Infrastructure network Gateway
infra_nw_gateway: 10.1.1.1
# Comma separated list of AZ's to be associated with this network
infra_nw_azs: PKS-MGMT 

# (true|false) to use nsx networking feature. PKS v1.1+ requires it to be true
nsx_networking_enabled: true
# CHANGE_ME Valid values: nsx-t, nsx-v 
nsx_mode: nsx-t 
# CHANGE_ME Nsx Mgr FQDN, not IP
nsx_address: nsxt-mgr-1.mylab.com
# CHANGE_ME username for nsx access 
nsx_username: admin 
# CHANGE_ME comment out if using vault
nsx_password: "VMware1!VMware1!" 

# Additional network
# Logical switch for PKS-K8S
pks_network_name: "PKS-K8S"
# vCenter PKS-K8S  network name for PKS - if empty quotes, then this network would be skipped
pks_vsphere_network: "LS-PKS-K8S"
# CHANGE_ME PKS-K8S network, usually VIP network for POC, CIDR, ex: 10.0.0.0/22
pks_nw_cidr: 10.2.2.0/24 
# CHANGE_ME PKS-K8S network exclusion range
pks_excluded_range: 10.2.2.1 
# CHANGE_ME PKS-K8S network DNS
pks_nw_dns: 10.173.61.54 
# CHANGE_ME PKS-K8S network Gateway
pks_nw_gateway: 10.2.2.1 
# Comma separated list of AZ's to be associated with this network
pks_nw_azs: PKS-K8S 
# Required select service network option in Ops man true or false
is_service_network: true 

## PKS Tile section ##
######################

# Create DNS entry in the loadbalancer and DNAT/SNAT entries for following
# CHANGE_ME Domain name only i.e - vmware.com
pks_tile_system_domain: mylab.com 
# Just provide the prefix like uaa or api for domain_prefix.
# The prefix together with system domain would be used like api.pks.test.corp.com or uaa.pks.test.corp.com
# Would be used for UAA as ${prefix}.${pks_system_domain}
pks_tile_uaa_domain_prefix: pks 

# IMPORTANT DNS RESOLUTION FOR PKS CONTROLLER
# External IP that would be routable via the NSX T0 Router and mapped to ${pks_tile_uaa_domain_prefix}.${pks_system_domain}
# Address from the PKS-MGMT network, usually in sequential access of bosh
pks_tile_uaa_system_domain_ip: 
pks_tile_cli_username: pksadmin
# CHANGE_ME comment out if using vault
pks_tile_cli_useremail: pksadmin@mylab.com # pksadmin@yourdomain.com
# Would be generated or provide
pks_tile_cert_pem: 
#cert_pem: |
  # -----BEGIN CERTIFICATE-----
  # MIIDjDCCAnSasfasdfsfd324242342UAMIG3232GMS4wLAYDVQQD
  # asfsafsafasf
  # .............
  # -----END CERTIFICATE-----

pks_tile_private_key_pem:

# All the following fields need to be filled in
# CHANGE_ME vcenter fqdn
pks_tile_vcenter_host: vcentersm.mylab.com
# CHANGE_ME vcenter username
pks_tile_vcenter_usr: administrator@vsphere.local
# CHANGE_ME Comment out if using vault
pks_tile_vcenter_pwd: "VMware1!"
# CHANGE_ME Datacenter where PKS control plane vm will be installed
pks_tile_vcenter_data_center: DC1 
# CHANGE_ME Cluster where PKS control plane vm will be installed
pks_tile_vcenter_cluster: COMP 
# CHANGE_ME Datastore where PKS control plane vm will be installed
pks_tile_vcenter_datastore: datastoreComp
# CHANGE_ME Folder where PKS control plane vm will be installed 
pks_tile_vm_folder: pks_vms 

# No changes  needed
pks_tile_singleton_job_az: PKS-MGMT 
pks_tile_nw_azs: PKS-MGMT
pks_tile_deployment_network_name: PKS-MGMT 
# Should match the PKS-K8S network created already
pks_tile_cluster_service_network_name: PKS-K8S 
pks_enable_cadvisor: true

# ALERT - The underlying Edge instances should be large, 8 vcpus.
# Otherwise the pks-nsx-t-precheck errrand would fail.
# can be true or false
pks_tile_nsx_skip_ssl_verification: true 

# WARNING!! dont disable PKS NSX-T Precheck Errand
# Tags get set as part of the errand for the Routers, IP Blocks, IP Pools etc.
# Default: false. Set to true if using medium edge sizes as PKS Errand would fail with smaller edges
pks_tile_disable_nsx_t_precheck_errand: false 

# PKS Main configs
# Name of T0 Router to be used for PKS
pks_tile_t0_router_name: T0RouterPKS1 
# Set to true
pks_tile_nsx_nat_mode: true 
# Name of VIP pool in nsx
pks_tile_external_ip_pool_name: pks-vip-pool 
# Name of Pod/Container IP Block in NSX Mgr to be used for PKS
pks_tile_container_ip_block_name: pks-pod-ip-block 
# Name of Node IP Block in NSX Mgr to be used for PKS nodes in v1.1
pks_tile_nodes_ip_block_name: pks-node-ip-block 
# CHANGE_ME List of vcenter clusters for PKS to deploy k8s clusters
pks_tile_vcenter_cluster_list: COMP 
# CHANGE_ME DNS server ip list for use by k8s nodes
pks_tile_nodes_dns_list: 10.173.61.54 

# Syslog Flags
# can be set to 'enabled', if 'disabled' all syslog properties ignored
pks_tile_syslog_migration_enabled: disabled
#101.10.10.10 
pks_tile_syslog_address: 
# 0
pks_tile_syslog_port: 
#tcp
pks_tile_syslog_transport_protocol: 
# true
pks_tile_syslog_tls_enabled: 
    # *.test.corp.com
pks_tile_syslog_peer: 
pks_tile_syslog_ca_cert:

# Allow public ip
# leave it to empty or false to turn off public ip
pks_tile_allow_public_ip: true 

# vRealize Log Insight (vrli) flags
# Change to true and fill following fields for using vrli
pks_tile_vrli_enabled: false 
pks_tile_vrli_host:
pks_tile_vrli_use_ssl: true
pks_tile_vrli_skip_cert_verify: true
# cert contents
pks_tile_vrli_ca_cert: 
pks_tile_vrli_rate_limit: 0

# PKS use Proxy
# Change to true and fill following fields for using proxy
pks_tile_enable_http_proxy: false 
pks_tile_http_proxy_url:
pks_tile_http_proxy_user:
pks_tile_http_proxy_password:
pks_tile_https_proxy_url:
pks_tile_https_proxy_user:
pks_tile_https_proxy_password:
pks_tile_no_proxy:

# Use LDAP for PKS UAA
# Default is to use internal uaa
# Change to true and fill following fields for using LDAP
pks_tile_uaa_use_ldap: false 
pks_tile_ldap_url:
pks_tile_ldap_user:
pks_tile_ldap_password:
pks_tile_ldap_search_base:
pks_tile_ldap_group_search_base:
pks_tile_ldap_server_ssl_cert:
pks_tile_ldap_server_ssl_cert_alias:
pks_tile_ldap_email_domains:
pks_tile_ldap_first_name_attribute:
pks_tile_ldap_last_name_attribute:
# Added
pks_tile_ldap_group_search_filter:
pks_tile_ldap_search_filter:
pks_tile_ldap_use_oidc:
pks_tile_admin_ad_group:
pks_tile_manage_ad_group:

# PKS Wavefront integration
# CHANGE_ME wavefront api url, empty is disabled
pks_tile_wavefront_api_url: https://surf.wavefront.com/api 
# CHANGE_ME wavefront access token
pks_tile_wavefront_token:   "64ac2da4-6949-4f8f-bbe9-651f5a227ff9" 
# CHANGE_ME email ids to sent alerts
pks_tile_wavefront_alert_targets:  torgersona@vmware.com 
# can be enabled or disabled
pks_tile_telemetry: disabled 

pks_tile_plan_details:
- plan_detail:
    name: "small"  # the name that appears for end users to choose
    plan_selector: plan1_selector # Dont change the value of selector - Needs to be planN_selector
    is_active: true
    description: "Small plan"
    # AZ can be only a single value for PKS v1.0
    # There can be a comma separated list for PKS v1.1 which supports multiple AZs
    az_placement: PKS-K8S  # Specify the AZ in which the cluster will be created, single for PKS v1.0
    master_vm_type: medium
    worker_vm_type: large.cpu
    persistent_disk_type: "10240" # Used in PKS 1.0
    master_persistent_disk_type: "10240" # Used in PKS 1.1+
    worker_persistent_disk_type: "10240" # Used in PKS 1.1+
    master_instances: 1    # The number of K8s worker instances
    worker_instances: 5    # The number of K8s worker instances
    errand_vm_type: micro
    addons_spec: "" # Kubernetes yml that contains specifications of addons to run on every cluster. This is an experimental feature. Please consider carefully before applying this to your plan
    # Privileged containers run with host-like permissions.
    # Allowing your users to deploy privileged containers in clusters using this plan can create security vulnerabilities and may impact other tiles.
    # Use with caution.
    allow_privileged_containers: true # default false
    # The admission controller will deny exec and attach commands to pods that run with escalated privileges that allow host access.
    # If checked, the admission controller will be disabled.
    # Clusters in this plan can then create security vulnerabilities and may impact other tiles.
        # Use with caution.
- plan_detail:
    name: "medium"  # the name that appears for end users to choose
    is_active: false
    plan_selector: plan2_selector # Dont change the value of selector - Needs to be planN_selector
    description: "Medium plan"
    # AZ can be only a single value for PKS v1.0
    # There can be a comma separated list for PKS v1.1 which supports multiple AZs
    az_placement: PKS-K8S  # Specify the AZ in which the cluster will be created, single for PKS v1.0
    master_vm_type: medium
    worker_vm_type: medium
    persistent_disk_type: "10240" # Used in PKS 1.0
    master_persistent_disk_type: "10240" # Used in PKS 1.1+
    worker_persistent_disk_type: "10240" # Used in PKS 1.1+
    master_instances: 3    # The number of K8s worker instances
    worker_instances: 5    # The number of K8s worker instances
    errand_vm_type: micro
    addons_spec: "" # Kubernetes yml that contains specifications of addons to run on every cluster. This is an experimental feature. Please consider carefully before applying this to your plan
    allow_privileged_containers: false  # Privileged containers run with host-like permissions. Allowing your users to deploy privileged containers in clusters using this plan can create security vulnerabilities and may impact other tiles. Use with caution."
    # The admission controller will deny exec and attach commands to pods that run with escalated privileges that allow host access.
    # If checked, the admission controller will be disabled.
    # Clusters in this plan can then create security vulnerabilities and may impact other tiles.
    # Use with caution.
- plan_detail:
    name: "large"  # the name that appears for end users to choose
    is_active: false
    plan_selector: plan3_selector # Dont change the value of selector - Needs to be planN_selector
    # other fields not considered if its not active

# Use this for downloading PKS from a s3 bucket
# s3_bucket: pks-tile-s3              # Required for S3. ID of the AWS S3 bucket to download Pivotal releases from
# s3_creds_access_key_id: AAa23....asdfZA       # Required for S3. Access key of the AWS S3 bucket
# s3_creds_secret_access_key: s6A234...asfsadUL   # Required for S3. Secret access key of the AWS S3 bucket
# s3_region: us-west-2                 # The region the bucket is in. Leave it blank if not applicable (e.g. for Minio)
# s3_pks_tile_path: pivotal-container-service-(.*).pivotal    # file path name to the tile in the s3 bucket

# Use this for downloading PKS tile from a web server (till it becomes available on Pivnet)
# pks_tile_webserver: http://101.101.101.101:8080      # EDIT ME
# pksv11_tile: pivotal-container-service-1.1.0.pivotal # EDIT ME

######## PKS tile-specific parameters
product_name: harbor-container-registry  # do not change
product_slug: harbor-container-registry  # do not change
#harbor_product_version: ^1\.7\..*$  # PKS tile version to install
harbor_product_version: 1.8.1  # PKS tile version to install
product_globs: "*.pivotal"

# Note: check companion "pks_vault_params.sh" script for automated credentials
#       creation for this pipeline in either Vault or CredHub.


######## AZs and Networks configuration for the tile
harbor_networks: |
  network:
    name: PKS-MGMT
  other_availability_zones:
  - name: PKS-MGMT
  - name: PKS-MGMT
  singleton_availability_zone:
    name: PKS-MGMT
harbor_tile_app_domain: mylab.com # CHANGE_ME - Make sure this matches the property specified in the block below
harbor_tile_app_domain_ip:

harbor_properties: |
  ######## General
  ### The FQDN (not IP) of Harbor instance. Its domain must match the wildcard domain used for generating Harbor certificate.
  .properties.hostname:
    value: harbor.mylab.com
  ######## SSL certificate and private key for Harbor
  ### SSL Certificate and key PEMs
  .properties.server_cert_key:
  #
  # The "generate_cert_domains" parameter controls the automatic certificates
  #   generation behavior for this property.
  #
  # If auto-generation of certs is desired, leave this parameter un-commented
  #   and update the array of domain names to be used for cert generation.
  #   e.g. ["harbor.mydomain.com"].
  #   Leave parameters cert_pem and private_key_pem with empty values in this case.
  #
  # Otherwise, either comment out or delete this parameter line and
  #   provide the certificate (cert_pem) and private key (private_key_pem) values
  #   in the corresponding parameters further below.
  #
    generate_cert_domains: ["harbor.mylab.com"]
    value:
      cert_pem:
      private_key_pem:
  ### Certificate Authority (CA) for the server certificate. If using self-signed certificate, please paste the corresponding CA here. Leave empty if using root CA of Ops Manager.
  .properties.server_cert_ca:
    value:
  ######## Credentials
  ### The password for the system administrator
  .properties.admin_password:
    value:
      secret: VMware1!
  ######## Authentication Mode
  ### Options:
  ### - db_auth        (Internal, i.e. Harbor internal user management)
  ### - ldap_auth      (LDAP)
  ### - uaa_auth_pks   (UAA in Pivotal Container Service)
  ### - uaa_auth_pas   (UAA in Pivotal Application Service)
  .properties.auth_mode:
    value: db_auth
  ### if "ldap_auth" is chosen for authentiaction mode, then uncomment and fill out the ldap parameters below
  ### otherwise, leave this section commented out to avoid Ops Manager parameter settings errors
  ###
  ### The LDAP Endpoint URL.
  # .properties.auth_mode.ldap_auth.url:
  #   value: ( ( ldap_auth_url ) )
  ### Verify LDAP server SSL certificate (flag) - true or false
  # .properties.auth_mode.ldap_auth.verify_cert:
  #   value: ( ( ldap_auth_verify_cert ) )
  ### The DN of the user who has the permission to search the LDAP server.
  # .properties.auth_mode.ldap_auth.searchdn:
  #   value: ( ( ldap_auth_searchdn ) )
  ### The password of the user who has the permission to search the LDAP server.
  # .properties.auth_mode.ldap_auth.searchpwd:
  #   value:
  #     secret: ( ( ldap_auth_searchpwd ) )
  ### The base DN from which to look up a user in LDAP server
  # .properties.auth_mode.ldap_auth.basedn:
  #   value: ( ( ldap_auth_basedn ) )
  ### The attribute used in a search to match a user, it could be uid, cn, email, sAMAccountName or other attributes depending on your LDAP server
  # .properties.auth_mode.ldap_auth.uid:
  #   value: ( ( ldap_auth_uid ) )
  ### Search filter for LDAP server. Make sure the syntax of the filter is correct
  # .properties.auth_mode.ldap_auth.filter:
  #   value: ( ( ldap_auth_filter ) )
  ### The LDAP scope to search for users
  ### Options:
  ### - 0        (Base)
  ### - 1        (nsx_networking_enabledevel)
  ### - 2        (Subtree)
  # .properties.auth_mode.ldap_auth.scope:
  #   value: ( ( ldap_auth_scope ) )
  ### The timeout (in seconds) when connecting to the LDAP Server
  # .properties.auth_mode.ldap_auth.timeout:
  #   value: ( ( ldap_auth_timeout ) )
  ######## Container Registry Storage
  ### Container Registry's filesystem
  ### Options:
  ### - filesystem     (Local File System)
  ### - s3             (AWS S3)
  .properties.registry_storage:
    value: filesystem
  ### if "s3" is chosen for Container Registry Storage, then uncomment and fill out the s3 parameters below
  ### otherwise, leave this section commented out to avoid Ops Manager parameter settings errors
  ###
  ### S3 Access Key
  # .properties.registry_storage.s3.access_key:
  #   value: ( ( s3_registry_storage_access_key ) )
  ### S3 Secret Key
  # .properties.registry_storage.s3.secret_key:
  #   value:
  #     secret: ( ( s3_registry_storage_secret_key ) )
  ### S3 Region
  # .properties.registry_storage.s3.region:
  #   value: ( ( s3_registry_storage_region ) )
  ### S3 Endpoint URL of your S3-compatible file store
  # .properties.registry_storage.s3.endpoint_url:
  #   value: ( ( s3_registry_storage_endpoint_url ) )
  ### S3 Bucket Name
  # .properties.registry_storage.s3.bucket:
  #   value: ( ( s3_registry_storage_bucket ) )
  ### S3 Root Directory in the Bucket
  # .properties.registry_storage.s3.root_directory:
  #   value: ( ( s3_registry_storage_root_directory ) )
  ######## Clair Settings
  ### Determine if include Clair in the deployment to support vulnerability scanning (flag: true or false)
  .properties.with_clair:
    value: true
  ######## with_notary Settings
  ### Determine if include Notary in the deployment to support content trust (flag: true or false)
  .properties.with_notary:
    value: true
######## Errands to disable
harbor_errands_to_disable: ""
  # smoke-testing, uaa-deregistration

######## Resources
harbor_resources: |
  harbor-app:
    instance_type:
      id: large.disk
    persistent_disk:
      size_mb: "102400"
  smoke-testing:
    instance_type:
      id: medium
